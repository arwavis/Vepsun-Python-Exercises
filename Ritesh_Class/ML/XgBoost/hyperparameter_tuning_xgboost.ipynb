{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamter Tuning - XGBoost\n",
    "\n",
    "# Here is the highlight of the process\n",
    "# 1. Import the customer churn data (I have already cleaned it)\n",
    "# 2. Split the data into test and train sets\n",
    "# 3. Build data matrices - as XGBoost uses DMatrix\n",
    "# 4. Find the logloss of the model with default parameters\n",
    "# 5. Tune the parameters\n",
    "# 6. Find the logloss of the model with tuned parameters\n",
    "\n",
    "# For exploratory analysis and other models on this dataset, please use the following link\n",
    "# https://github.com/Nickssingh/Churn-Prediction-Model-Telecommunication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import the dataset and view top rows\n",
    "# I have already preapared the the data for analysis \n",
    "    # Removed the missing values\n",
    "    # Converted the variables into appropriate data types\n",
    "    # Encoded categorical variables using one hot encoding\n",
    "\n",
    "df_churn=pd.read_csv(\"Data/telcom_customer_churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimension of the data\n",
    "\n",
    "df_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test datasets\n",
    "# test:train = 3:7\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_temp = df_churn\n",
    "y = df_temp['Churn']\n",
    "X = df_temp.drop('Churn', axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing XGBoost\n",
    "\n",
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost uses an internal data structure DMatrix - which optimizes both memory effieciency and speed\n",
    "# Hence, rather than using pandas dataframe, we will use data matrix - DMatrix\n",
    "import xgboost as xgb\n",
    "dm_train = xgb.DMatrix(X_train, label=y_train)\n",
    "dm_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Model\n",
    "\n",
    "# Ideal case would include an exhaustive gridsearch on all the parameters.\n",
    "# However, such an approach is computationally intensive.\n",
    "# Hence, we will focus on few important parameters and tune them sequentially.\n",
    "\n",
    "# Following are the parameters that we will tune in this process\n",
    "# max_depth\n",
    "# min_child_weight\n",
    "# subsample\n",
    "# colsample_bytree\n",
    "# eta\n",
    "# num_boost_rounds\n",
    "# early_stopping_rounds\n",
    "\n",
    "# We will use logistic loss function to assess the accuracy of predictions, as this is a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will set num_boost_rounds to 100, early_stopping_rounds to 10, and objective to binary:logistic.\n",
    "# All the other values at this stage are default values.\n",
    "# We will tune our model by chaning the default values.\n",
    "\n",
    "params = {'max_depth':6, 'min_child_weight':1, 'eta':0.3, 'subsample':1, \n",
    "          'colsample_bytree':1, 'objective':'binary:logistic',}\n",
    "\n",
    "# We will use logloss function to evaluate the model's performance\n",
    "params['eval_metric'] = \"logloss\"\n",
    "\n",
    "xgmodel = xgb.train(params, dtrain = dm_train, num_boost_round = 100, evals = [(dm_test,\"Test\")], \n",
    "                    early_stopping_rounds = 10)\n",
    "\n",
    "print(\"Best Logloss: {:.3f} | Rounds: {}\".format(xgmodel.best_score,xgmodel.best_iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we found that the tenth round gave the best result and the results did not improve in the next 10 rounds\n",
    "# Hence, the iteration stopped at round 19 and we did not reach the maximum number of boosting rounds (100).\n",
    "\n",
    "# Finding a suitable evidence to stop the iterations is important.\n",
    "# Stopping the iterations when results do not improve prevents overfittig and the inefficient utilization of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use cross validation to tune the parameters within the params dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters: max-depth and min_child_weight\n",
    "# I realized that the optimal values are in the following ranges through multiple iterations\n",
    "\n",
    "gridsearch_params = [(max_depth, min_child_weight)\n",
    "                    for max_depth in range(1,4)\n",
    "                    for min_child_weight in range(17,21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_min = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    \n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    \n",
    "    xg_cvresults = xgb.cv(params, dtrain = dm_train, num_boost_round = 100,\n",
    "                      seed = 0, nfold=10, metrics = {'logloss'}, early_stopping_rounds = 10,)\n",
    "    \n",
    "    logloss_mean = xg_cvresults['test-logloss-mean'].min()\n",
    "    \n",
    "    print(\"max_depth: {} | min_child_weight: {} with Logloss: {:.3}\\n\".format(max_depth,min_child_weight,logloss_mean))\n",
    "    \n",
    "    if logloss_mean < logloss_min:\n",
    "        logloss_min = logloss_mean\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "\n",
    "        \n",
    "print(\"Best Parameters: max_depth: {} | min_child_weight: {} with Logloss: {:.3f}\". format(best_params[0], \n",
    "                                                                                  best_params[1], logloss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the parameters with the best values: max_depth = 2 and min_child_weight = 19\n",
    "\n",
    "params['max_depth'] = 2\n",
    "params['min_child_weight'] = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters: subsample and colsample_bytree\n",
    "# I found that the optimal values are in the following ranges through multiple iterations\n",
    "\n",
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(1,5)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_min = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for subsample, colsample in (gridsearch_params):\n",
    "    \n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "    \n",
    "    xg_cvresults = xgb.cv(params, dtrain = dm_train, num_boost_round = 100,\n",
    "                      seed = 0, nfold=10, metrics = {'logloss'}, early_stopping_rounds = 10,)\n",
    "    \n",
    "    logloss_mean = xg_cvresults['test-logloss-mean'].min()\n",
    "    \n",
    "    print(\"subsample: {} | colsample: {} with Logloss: {:.3f}\\n\".format(subsample,colsample,logloss_mean))\n",
    "    \n",
    "    if logloss_mean < logloss_min:\n",
    "        logloss_min = logloss_mean\n",
    "        best_params = (subsample, colsample)\n",
    "        \n",
    "print(\"Best Parameters: subsample: {} | colsample: {} with Logloss: {:.3f}\". format(best_params[0], \n",
    "                                                                           best_params[1], logloss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the parameters with the best values: subsample = 0.9 and colsample = 0.4\n",
    "\n",
    "params['subsample'] = 0.9\n",
    "params['colsample_bytree'] = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter: eta\n",
    "\n",
    "logloss_min = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [0.3, 0.2, 0.1, 0.05, 0.01, 0.005]:\n",
    "    \n",
    "    params['eta'] = eta\n",
    "    \n",
    "    xg_cvresults = xgb.cv(params, dtrain = dm_train, num_boost_round = 100,\n",
    "                      seed = 0, nfold=10, metrics = {'logloss'}, early_stopping_rounds = 10,)\n",
    "    \n",
    "    logloss_mean = xg_cvresults['test-logloss-mean'].min()\n",
    "    print(\"eta: {} with Logloss: {:.3}\\n\".format(eta,logloss_mean))\n",
    "    \n",
    "    if logloss_mean < logloss_min:\n",
    "        logloss_min = logloss_mean\n",
    "        best_params = eta\n",
    "        \n",
    "print(\"Best Parameter: eta: {} with Logloss: {:.3f}\". format(best_params, logloss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the eta parameter with the best value\n",
    "\n",
    "params['eta'] = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the optimum paramters\n",
    "\n",
    "params = {'colsample_bytree': 0.4,\n",
    "          'eta': 0.3,\n",
    "          'eval_metric': 'logloss',\n",
    "          'max_depth': 2,\n",
    "          'min_child_weight': 19,\n",
    "          'objective':'binary:logistic',\n",
    "          'subsample': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(eta=0.3,max_depth=2,colsample_bytree=0.4,subsample=0.9,min_child_weight=19,\n",
    "                      objective='binary:logistic')\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the optimal number of rounds for the model with new parameters\n",
    "\n",
    "xgmodel_tuned = xgb.train(params, dtrain = dm_train, \n",
    "                          num_boost_round=100, evals=[(dm_test,\"Test\")], early_stopping_rounds=10)\n",
    "\n",
    "\n",
    "print(\"Best Logloss: {:.3f} in {} rounds\". format(xgmodel_tuned.best_score, xgmodel_tuned.best_iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the tuned parameters we would need 27 rounds to achieve the best result\n",
    "\n",
    "# The improvement after parameter tuning is marginal in our case.\n",
    "    # Logloss of our model decreased from 0.424 to 0.417\n",
    "# However, we were able to see how parameters can be tuned.\n",
    "\n",
    "# Here we have used only a few combination of parameters.\n",
    "# We can further improve the impact of tuning; however, doing so would be computationally more expensive.\n",
    "# More combination of parameters and wider ranges of values for each of those paramaters would have to be tested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
